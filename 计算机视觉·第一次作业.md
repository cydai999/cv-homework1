# 计算机视觉·第一次作业

## 姓名： 代承谕

## 学号： 22307130165

[TOC]

### 一. 介绍

本项目仅用`numpy`实现了简单的三层全连接神经网络，包含网络，训练，测试和超参数查找四个部分， 支持自定义隐藏层大小和激活函数类型。其中，模型部分（`mynn`）包含网络层（`layers.py`），损失函数（`loss_fn`），优化器（`optimizers` ），学习率调整器（`lr_scheduler`），评价标准（`metric.py`），模型架构（`models.py`）和运行器（`runner.py`）。

训练部分（`train.py`）支持在CIFAR-10数据集上进行训练，训练时以5：1划分训练集（train set）与验证集（valid set）,训练完毕后在测试集（test set）上计算精度，并支持保存每个epoch的模型及最优模型的参数，所用的超参数，损失/精度-迭代次数图像及测试结果。

测试部分（`test.py`）支持读取保存的模型参数，并在CIFAR-10的测试集上进行测试。

超参数查找部分（`hyperparam_search.py`） 在一个随机选取的0.1x样本上对不同的学习率（init_lr），隐藏层维度（hidden_size），权重衰减系数（weight_decay_param）进行测试，分别计算不同组合在CIFAR-10测试集上的损失和精度。

本报告主要介绍了模型架构，数据集，实验过程，实验结果及分析。

本项目的代码见https://github.com/cydai999/cv-homework1.git

本项目的模型参数和所用数据集（CIFAR-10）可以在https://drive.google.com/drive/folders/1o9RS29zpJ1bYAaT2TJ2hgZOFu_YXL77V?usp=drive_link下载。



### 二. 模型

本项目中实现的网络为简单的三层全连接神经网络，包含一个输入层，一个隐藏层和一个输出层。其中，输入层固定为3072个

神经元（输入样本的尺度），输出层有10个神经元（分类数），隐藏层维度可调。网络架构如下图所示：

![神经网络示意图](D:\学习\大三\下学期\计算机视觉\cv-homework1\神经网络示意图.jpg)

本模型包含两个线性层，在第一个线性层之后，设置了可选的激活函数以增强模型的拟合能力；在第二个线性层后，设置了softmax函数以实现归一化。

在`mynn`模块中，`layers.py`中实现了线性层与Sigmoid， ReLU和LeakyReLU激活函数的前向传播与反向传播过程；`loss_fn.py`实现了交叉熵损失，及其反向传播过程；`optimizer.py`中分别实现了随机梯度下降（SGD）及带动量的梯度下降（SGDMomentum）方法；`lr_schduler.py`中实现了不同的学习率调整策略，包含StepLR，MultiStepLR和ExponentialLR；`metric.py`中实现了精度（accuracy）计算；`models.py`实现了网络层拼接，前向/反向传播，以及保存/加载模型的方法；`runner.py`实现了模型的训练和评测功能。



### 三. 数据集

本次使用的数据集为**CIFAR-10**，是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集，一共包含10 个类别的RGB 彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。数据集中每个图片的尺寸为32 × 32 ，每个类别有6000个图像，共有50000 张训练图片和10000 张测试图片。

<img src="D:\学习\大三\下学期\计算机视觉\cv-homework1\Cifer-Data_Enhanced-1.jpg" alt="A simple CNN In TensorFlow: Practical CIFAR-10 Guide" style="zoom: 50%;" />

由于CIFAR-10中的图片为彩色图像，其分类难度比MNIST手写数字数据集难度更高。本次实验中也观察到，全连接神经网络的分类精度仅在0.3-0.4之间。



### 四. 实验过程

这一部分主要介绍查找最佳超参数的过程和训练过程。

#### （1）参数查找

查找过程分为三个阶段：

1. 选择最优激活函数
2. 选择最优优化器
3. 查找最优超参数

以下分别介绍这几个过程。

1. 选择最优激活函数：

   为了观察不同激活函数的性能，在其他超参数相同的情况下分别对以Sigmoid, ReLU和LeakyReLU作为激活函数的网络进行训练与评测。所用超参数如下所示：

   ```yaml
   {"model": {"size_list": [3072, 1000, 10], "weight_decay_list": [1e-05, 1e-05]}, "optimizer": {"type": "sgd", "init_lr": 0.001}, "lr_scheduler": {"type": "stepLR", "step_size": 1, "gamma": 0.5}, "metric": {"type": "accuracy"}, "loss function": {"type": "cross entropy"}, "train": {"batch_size": 32, "epoch": 5, "log_iter": 100}}
   ```

   结果如下：

   **Sigmoid**

   ```yaml
   {"loss": 2.0212380363599647, "score": 0.2822}
   ```

   **ReLU**

   ```yaml
   {"loss": 2.045286404788069, "score": 0.322}
   ```

   **LeakyReLU**

   ```yaml
   {"loss": 2.045663141204463, "score": 0.3217}
   ```

   可以观察到，Sigmoid激活函数的效果最差，而ReLU与LeakyReLU的效果相近。而LeakyReLU可以解决可能存在的dead ReLU问题，因此之后选用LeakyReLU作为激活函数。

2. 选择最优优化器：

   为了观察不同优化器的性能，在其他超参数相同的情况下分别使用SGD优化器和SGDMomentum优化器对神经网络进行训练和评测。所用超参数如下所示：

   ```
   {"model": {"size_list": [3072, 1000, 10], "act_func": "LeakyReLU", "weight_decay_list": [1e-05, 1e-05]}, "optimizer": {"init_lr": 0.001}, "lr_scheduler": {"type": "stepLR", "step_size": 1, "gamma": 0.5}, "metric": {"type": "accuracy"}, "loss function": {"type": "cross entropy"}, "train": {"batch_size": 32, "epoch": 5, "log_iter": 100}}
   ```

   结果如下：

   **SGD**

   ```
   {"loss": 2.045663141204463, "score": 0.3217}
   ```

   **SGDMomentum**

   ```
   {"loss": 1.6221552291452583, "score": 0.4301}
   ```

   可以观察到，带动量的SGD优化器其效果显著优于不带动量的SGD优化器。这可能是因为，动量机制的引入使得优化过程能更快脱离      局部最优点或鞍点， 同时加快收敛速度。因此，之后选用SGDMomentum作为优化器。

3. 查找最优超参数：

   选定激活函数与优化器之后，可以开始对超参数的查找。查找过程涉及到四个超参数：学习率（init_lr），隐藏层维度（hidden_size），权重衰减系数（weight_decay_param）和学习率衰减系数（gamma）。实际查找的过程可以分为两个阶段：首先查找学习率，隐藏层维度和权重衰减系数的最优组合，然后选取不同的学习率衰减系数观察模型性能。

   a) 组合查找学习率，隐藏层维度和权重衰减系数：
   
   每个超参数可能的取值如下所示：
   
   ```python
   init_lrs: [1e-2, 1e-3, 1e-4]
   hidden_size: [2000, 1000, 500]
   weight_decay_param: [1e-3, 1e-4, 1e-5]
   ```
   
   具体实现过程是对这三个参数每个可能的组合对应的网络在相同数据上进行训练和测试。由于模型数量过多，没有在整个数据集上训练，而是随机选取了一个0.1x的样本数据集。其他超参数如下所示：

   ```yaml
   {"model": {"act_func": "LeakyReLU"}, "optimizer": {"type": "momentum"}, "lr_scheduler": {"type": "stepLR", "step_size": 5, "gamma": 0.8}, "metric": {"type": "accuracy"}, "loss function": {"type": "cross entropy"}, "train": {"batch_size": 32, "epoch": 10, "log_iter": 100}}
   ```
   
   所有参数组合在测试集上的损失和精度如下：
   
   | init_lr | hidden_size | weight_decay_param | loss | accuracy |
   | :-------: | :-----------: | :------------------: | :----: | :--------: |
   |1e-2|500|1e-3| 1.833 |  0.3500  |
   |1e-2|500|1e-4|1.806|0.3678|
   |1e-2|500|1e-5|1.834|0.3667|
   |1e-2|1000|1e-3|1.897|0.371|
   |1e-2|1000|1e-4|1.882|0.3539|
   |1e-2|1000|1e-5|1.811|0.3705|
   |1e-2|2000|1e-3|1.863|0.3683|
   |1e-2|2000|1e-4|1.828|0.3703|
   |1e-2|2000|1e-5|1.812|0.3854|
   | 1e-3 | 500 | 1e-3 | 1.830 | 0.3569 |
   | 1e-3 | 500 | 1e-4 | 1.806 | 0.3708 |
   | 1e-3 | 500 | 1e-5 | 1.830 | 0.3607 |
   | 1e-3 | 1000 | 1e-3 | 1.973 | 0.3485 |
   | 1e-3 | 1000 | 1e-4 | 2.017 | 0.3305 |
   | 1e-3 | 1000 | 1e-5 | 1.891 | 0.3701 |
   | 1e-3 | 2000 | 1e-3 | 2.031 | 0.3631 |
   | 1e-3 | 2000 | 1e-4 | 1.971 | 0.3823 |
   | 1e-3 | 2000 | 1e-5 | 2.069 | 0.3699 |
   | 1e-4 | 500 | 1e-3 | 2.105 | 0.2856 |
   | 1e-4 | 500 | 1e-4 | 2.099 | 0.2829 |
   | 1e-4 | 500 | 1e-5 | 2.174 | 0.2750 |
   |1e-4|1000|1e-3|2.253|0.2843|
   |1e-4|1000|1e-4|2.270|0.2841|
   |1e-4|1000|1e-5|2.290|0.2754|
   |1e-4|2000|1e-3|2.556|0.2776|
   |1e-4|2000|1e-4|2.592|0.2831|
   |1e-4|2000|1e-5|2.620|0.2758|
   
   **分析：**
   
   1. **init_lr**:
   
      较高的学习率可以加快收敛速度，但会导致训练过程出现震荡，可能无法达到最优点；较低的学习收敛速度慢，需要更长时间才能收敛，但训练过程更加稳定。在实验中，取学习率为1e-4时模型性能远低于1e-2和1e-3时。
   
   2. **hidden_size**:
   
      在其他参数相同的情况下，隐藏层维数越高，模型参数越多，其拟合能力就越强，但训练开销也越大，同时可能导致过拟合问题。在实验中，观察到隐藏层维度越高，在测试集上的精度越高，但收益较低（以学习率取1e-3为例，隐藏层取2000维比1000维只能带来较低的精度提升甚至负提升，如下表所示）。
      | init_lr | hidden_size | weight_decay_param | loss | accuracy |
      | :-------: | :-----------: | :------------------: | :----: | :--------: |
      | 1e-3 | 500 | 1e-3 | 1.830 | 0.3569 |
      |  1e-3   |    1000     |        1e-3        | 1.973 |  0.3485  |
      |  1e-3   |    2000     |        1e-3        | 2.031 |  0.3631  |
      |  1e-3   |     500     |        1e-4        | 1.806 |  0.3708  |
      | 1e-3 | 1000 | 1e-4 | 2.017 | 0.3305 |
      |  1e-3   |    2000     |        1e-4        | 1.971 |  0.3823  |
      |  1e-3   |     500     |        1e-5        | 1.830 |  0.3607  |
      |  1e-3   |    1000     |        1e-5        | 1.891 |  0.3701  |
      |  1e-3   |    2000     |        1e-5        | 2.069 |  0.3699  |
   
   3. **weight_decay_param**:
   
      较大的权重衰减参数意味着更强的泛化能力，但可能导致模型本身的拟合能力较弱，因此选择时需要综合考虑其他因素（学习率和隐藏层维度）。
      
      
      
      综合考虑模型性能与训练开销，最终选择的超参数组合如下：
      
      ```
      init_lr: 1e-2
      hidden_size: 1000
      weight_decay_param: 1e-5
      ```
   
   b) 查找学习率衰减系数
   
   在固定其他参数的请况下，选用不同的学习率衰减系数，观察模型性能。所用学习率衰减系数如下所示：
   
   ```
   gammas: [1, 0.5, 0.1, 0.05]
   ```
   
   其他超参数如下所示：
   
   ```yaml
   {"model": {"size_list": [3072, 1000, 10], "act_func": "LeakyReLU", "weight_decay_list": [1e-05, 1e-05]}, "optimizer": {"type": "momentum", "init_lr": 0.01}, "lr_scheduler": {"type": "stepLR", "step_size": 5}, "metric": {"type": "accuracy"}, "loss function": {"type": "cross entropy"}, "train": {"batch_size": 32, "epoch": 20, "log_iter": 100}}
   ```
   
   结果如下表：
   | gamma | loss | accuracy |
   | :-----: | :----: | :--------: |
   | 1     | 1.989 | 0.3798 |
   | 0.5 | 1.757 | 0.4080 |
   | 0.1 | 1.694 | 0.4044 |
   | 0.05 | 1.718 | 0.3945 |
   
   可以观察到，学习率调整机制起到了一定的效果。之后选用gamma=0.1进行训练。

#### （2）模型训练
选用如下配置对模型进行训练：

```yaml
{"model": {"size_list": [3072, 1000, 10], "act_func": "LeakyReLU", "weight_decay_list": [1e-05, 1e-05]}, "optimizer": {"type": "momentum", "init_lr": 0.01}, "lr_scheduler": {"type": "stepLR", "step_size": 5, "gamma": 0.1}, "metric": {"type": "accuracy"}, "loss function": {"type": "cross entropy"}, "train": {"batch_size": 32, "epoch": 20, "log_iter": 100}}
```

训练过程中的精度损失可视化如下：

![loss_accuracy_plot](D:\学习\大三\下学期\计算机视觉\cv-homework1\saved_models\best_model\loss_accuracy_plot.png)

最终结果如下所示：

```yaml
{"loss": 1.4387048702457617, "score": 0.4908}
```

模型参数可视化如下图所示（仅可视化第一个线性层参数，即3024*1000的矩阵）：

![image-20250412162045888](C:\Users\代承谕\AppData\Roaming\Typora\typora-user-images\image-20250412162045888.png)

**分析：**

观察右侧的精度-迭代次数图像，可以看到模型在验证集上的精度总体保持上升趋势，刚开始上升速度较快，之后逐渐趋于平缓。值得注意的是，在第6000个iteration左右精度出现跃升现象，这是由于在该处发生了学习率衰减，变为了先前的0.1倍，导致模型能够更好地找到极值点。

上图是对第一个线性层权重参数的可视化，可以看到其内部并无明显特征。这可能是因为全连接层难以直接捕捉图像数据中的特征，要想得到更明显的特征需要使用卷积神经网络（CNN）。



### 五. 总结与思考

本项目实现了简单的三层全连接神经网络，在CIFAR-10数据集上进行训练和测试， 并实现了基本的超参数查找过程。由于该神经网络层数过少，架构简单，仅使用全连接层等原因，模型在测试集上的精度仅为0.3-0.4左右。之后可能的改进方向有：

1. 增加神经网络层数。更深的神经网络可能会拥有更好的拟合性能。
2. 将全连接层替换为卷积层和池化层。由于输入数据为图像数据，卷积神经网络可能可以更好地捕捉数据中的特征。同时，卷积神经网络参数量更少，可以防止过拟合。
3. 使用数据增强技术，获得更多有效的样本数据。
4. 使用更复杂的网络架构，如添加Inception模块或使用残差连接技术。
5. 使用随机失活技术（dropout）以防止过拟合。

 	

​						